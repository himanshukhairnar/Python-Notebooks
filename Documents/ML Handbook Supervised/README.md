## Contents

|[Regression](#Regression)|[Classification](#Classification)|[Text Classification](#Text-Classification)|[Time Series](#Time-Series)|
|:-|:-|:-|:-|
|• [Linear Regression](#11)<br> • [Polynomial Regression](#12)<br> • [Quantile Regression](#13)<br> • [Ridge Regression](#14)<br> • [Lasso Regression](#15)<br> • [Support Vector Regression](#16)<br> • [Decision Tree - CART](#17)<br> • [Random Forest Regression](#18)<br> • [GBM](#19)<br> • [Stochastic Gradient Descent](#110)<br> • [KNN Regressor](#111)<br> • [XGB Regressor](#112)<br> • [LightGBM](#113)|• [Logistic Regression](#21)<br>• [Naïve Bayes](#22)<br>• [Support Vector Machine](#23)<br>• [Stochastic Gradient Descent](#24)<br>• [K-Nearest Neighbor](#25)<br>• [Decision Tree](#26)<br>• [Random Forest](#27)<br>• [Extra Trees Classifier](#28)<br>• [Gradient Boosting Classifier](#29)<br>• [XGBoost Classifier](#210)|• [(LINEAR)Support Vector Classification](#31)<br>• [Naïve Bayes](#32)<br>• [Logistic Regression](#33)|• [Autoregression (AR)](#41)<br>• [Moving Average (MA)](#42)<br>• [Auto Regressive Integrated Moving Average (ARIMA)](#43)<br>• [Seasonal Autoregressive Integrated Moving-Average (SARIMA)](#44)<br>• [Vector Autoregression (VAR)](#45)<br>• [ARIMAX/SARIMAX](#46)|

# <code style="background:yellow;color:black">Regression</code>		
"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a continuous dependent variable and one or more independent variables. 
Following are the Regression Algorithms widely used -"					
						
|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='11' ></a>**Linear Regression**|It is the simplest form of regression. It is a technique in which the dependent variable is continuous in nature.|**fit_intercept**, default=True Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).<br>**normalize**, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.|•  It is simple to implement and easier to interpret|•  Linear Regression Is Limited to Linear Relationships.<br>•  Data Must Be Independent<br>•  Linear Regression Is Sensitive to Outliers<br>•  Linear Regression Only Looks at the Mean of the Dependent Variable|
|<a id='12' ></a>**Polynomial Regression**|It is a technique to fit a nonlinear equation by taking polynomial functions of independent variable.|**degree**, default = 2 The degree of the polynomial features. <br>**interaction_only**, default = False If true, only interaction features are produced<br> **include_bias**, default = True If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model). <br>**orderstr** in {‘C’, ‘F’}, default ‘C’ Order of output array in the dense case. ‘F’ order is faster to compute, but may slow down subsequent estimators.|•  Polynomial provides the best approximation of the relationship between the dependent and independent variable.|•  A Broad range of function can be fit under it|•  Polynomial basically fits a wide range of curvature.|•  The presence of one or two outliers in the data can seriously affect the results of the nonlinear analysis<br>•  These are too sensitive to the outliers<br>•  In addition, there are unfortunately fewer model validation tools for the detection of outliers in nonlinear regression than there are for linear regression<br><br>[Go To Top](#Contents)|
|<a id='13' ></a>**Quantile Regression**|Quantile regression is the extension of linear regression and we generally use it when outliers, high skeweness and heteroscedasticity exist in the data.|In statsmodels’ QuantReg class, only parameter is q. Can be between (.05, .96)|•  Quite beneficial when heteroscedasticity is present in the data<br>•  Robust to outliers<br> •  Distribution of dependent variable can be described via various quantiles.<br> •  It is more useful than linear regression when the data is skewed|•  Increased computing time required for bootstrapping and other computational procedures<br>•  Some aspects of the method are still being developed and have not yet been implemented in mainstream statistical software|
|<a id='14' ></a>**Ridge Regression**|Ridge makes use of variable shrinkage,i.e., L2 regularization technique in the objective function.|**alpha**, default = 0.1, The alpha parameter controls the degree of sparsity of the estimated coefficients.|•  Large number of variables<br>•  Low ratio of number observations to number of variables<br>•  High Multi-Collinearity|•  Firstly ridge regression includes all the predictors in the final model, unlike the stepwise regression methods which will generally select models that involve a reduced set of variables<br>•  A ridge model does not perform feature selection. If a greater interpretation is necessary where we need to reduce the signal in our data to a smaller subset then a lasso model may be preferable<br>•  Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The lasso regression is an alternative that overcomes this drawback|
|<a id='15' ></a>**Lasso Regression**|Lasso makes use of variable selection, i.e., L1 regularization technique in the objective function.|**alpha**, default = 0.1,The alpha parameter controls the degree of sparsity of the estimated coefficients.|•  Works well with Large number of variables too<br> •  Low ratio of number observations to number of variables<br> •  High Multi-Collinearity|•  LASSO selects at most n variables before it saturates<br> •  LASSO can not do group selection. If there is a group of variables among which the pairwise correlations are very high, then the LASSO tends to arbitrarily select only one variable from the group<br><br><br>[Go To Top](#Contents)|
|<a id='16' ></a>**Support Vector Regression**|Support vector regression can solve both linear and non-linear models. SVM uses non-linear kernel functions (such as polynomial) to find the optimal solution for non-linear models.|<br>**kernel**{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’ Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix. <br>**degree**, default=3 Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels. **gamma**{‘scale’, ‘auto’} or float, default=’scale’ Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, if ‘auto’, uses 1 / n_features.|•  Can be used for both linear and non-linear models<br>•  It can be used when the number of features are large|•  SVM algorithm is not suitable for large data sets<br>•  SVM does not perform very well, when the data set has more noise i.e. target classes are overlapping<br>• It can be used when the number of features are large but in cases where number of features for each data point exceeds the number of training data sample , the SVM will under perform|
|<a id='17' ></a>**Decision Tree - CART**|Decision tree is a tree based algorithm used to solve regression and classification problems.| **criterion**{“mse”, “friedman_mse”, “mae”}, default=”mse” The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, and “mae” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node. <br>**splitter**{“best”, “random”}, default=”best” The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.<br>**max_depth**, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.<br>**min_samples_split**, default=2 The minimum number of samples required to split an internal node|Used when there is  collinearity, noise, variance in the dataset and also when the dataset is large.|•  A small change in the data can cause a large change in the structure of the decision tree causing instability<br>•  For a Decision tree, sometimes calculation can go far more complex compared to other algorithms<br>•  Decision tree training is relatively expensive as complexity and time taken is more<br>•  Tends to overfit unless a good pruning method is used|
|<a id='18' ></a>**Random Forest Regression**|Random Forest is a collection of decision trees and average/majority vote of the forest is selected as the predicted output.|**n_estimators**, default=100 The number of trees in the forest.<br> **criterion**{“mse”, “mae”}, default=”mse” The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.<br>**max_depth**, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.<br> **min_samples_split**, default=2 The minimum number of samples required to split an internal node|Used when there is  collinearity, noise, variance in the dataset and also when the dataset is large|•  Random Forest creates a lot of trees and combines their outputs. To do so, this algorithm requires much more computational power and resources. On the other hand decision tree is simple and does not require so much computational resources<br>•  Random Forest requires more time to train as compared to decision trees as it generates a lot of trees (instead of one tree in case of decision tree) and makes decision on the majority of votes<br>•  Inherently less interpretable than an individual decision tree<br><br><br>[Go To Top](#Contents)|
|<a id='19' ></a>**GBM**|In gradient boosting, the ensemble model, we try to build is also a weighted sum of weak learners. Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.|**loss**{‘ls’, ‘lad’, ‘huber’, ‘quantile’}, default=’ls’ loss function to be optimized. ‘ls’ refers to least squares regression. ‘lad’ (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).<br>**learning_rate**, default=0.1 learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.<br> **n_estimators**  , default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.<br> **subsample**, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.|Boosting Is mainly focused at reducing high bias in the data|•  GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting<br>•   Must use cross-validation to neutralize. Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive<br>•  The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning<br>•  Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.)|
|<a id='110' ></a>**Stochastic Gradient Descent**|Stochastic Gradient Descent (SGD) regressor basically implements a plain SGD learning routine supporting various loss functions and penalties to fit linear regression models.|**squared_loss** − It refers to the ordinary least squares fit.<br>**huber**: SGDRegressor − correct the outliers by switching from squared to linear loss past a distance of epsilon. The work of ‘huber’ is to modify ‘squared_loss’ so that algorithm focus less on correcting outliers.<br> **epsilon_insensitive** − Actually, it ignores the errors less than epsilon. <br>**squared_epsilon_insensitive** − It is same as epsilon_insensitive. The only difference is that it becomes squared loss past a tolerance of epsilon.|•  It is easier to fit into memory due to a single training sample being processed by the network<br>•  It is computationally fast as only one sample is processed at a time. For larger datasets it can converge faster as it causes updates to the parameters more frequently<br>•  Due to frequent updates the steps taken towards the minima of the loss function have oscillations which can help getting out of local minimums of the loss function (in case the computed position turns out to be the local minimum)|•  Due to frequent updates the steps taken towards the minima are very noisy. This can often lead the gradient descent into other directions<br>•  Also, due to noisy steps it may take longer to achieve convergence to the minima of the loss function<br>•  Frequent updates are computationally expensive due to using all resources for processing one training sample at a time<br>•  It loses the advantage of vectorized operations as it deals with only a single example at a time<br><br><br>[Go To Top](#Contents)|
|<a id='111' ></a>**KNN Regressor**|K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure.|**n_neighbors**int, default=5 Number of neighbors to use by default for kneighbors queries.<br> **weights**{‘uniform’, ‘distance’} or callable, default=’uniform’ weight function used in prediction. <br>**algorithm**{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’ Algorithm used to compute the nearest neighbors|•  It is easy to explain and understand/interpret<br>•  Accuracy is pretty high but not competitive in comparison to better supervised learning models<br>•  It is versatile, so is useful for classification or regression|•  It is computationally expensive as it stores all of the training data<br>•  It has a high memory requirement<br>•  Stores all (or almost all) of the training data<br>•  Prediction stage might be slow (with big N)<br>•  Sensitive to irrelevant features and the scale of the data|
|<a id='112' ></a>**XGB Regressor**|XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.|**eta**[default=0.3] Analogous to learning rate in GBM Makes the model more robust by shrinking the weights on each step<br>**min_child_weight** [default=1] Defines the minimum sum of weights of all observations required in a child.Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. Too high values can lead to under-fitting hence, it should be tuned using CV.<br>**max_depth** [default=6] The maximum depth of a tree, same as GBM. Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample. Should be tuned using CV.<br>**max_leaf_nodes** The maximum number of terminal nodes or leaves in a tree. Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves. If this is defined, GBM will ignore max_depth.|•  Regularization: XGBoost has in-built L1 (Lasso Regression) and L2 (Ridge Regression) regularization which prevents the model from overfitting. That is why, XGBoost is also called regularized form of GBM (Gradient Boosting Machine)<br>•  Parallel Processing: XGBoost utilizes the power of parallel processing and that is why it is much faster than GBM. It uses multiple CPU cores to execute the model<br>•  Handling Missing Values: XGBoost has an in-built capability to handle missing values. When XGBoost encounters a missing value at a node, it tries both the left and right hand split and learns the way leading to higher loss for each node. It then does the same when working on the testing data<br>•  Cross Validation: XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. This is unlike GBM where we have to run a grid-search and only a limited values can be tested<br>•  Effective Tree Pruning: A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain|•  In Xgboost, you have to manually create dummy variable/ label encoding for categorical features before feeding them into the models|
|<a id='113' ></a>**LightGBM**|Light GBM is a gradient boosting framework that uses tree based learning algorithm. Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.|**num_leaves**:This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.<br> **min_data_in_leaf**: This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and num_leaves. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.<br> **max_depth**: You also can use max_depth to limit the tree depth explicitly. Training efficiency, low memory usage, high accuracy, parallel learning, corporate support, and scale-ability.	LightGBM has narrow user base as it is relatively new| Training efficiency, low memory usage, high accuracy, parallel learning, corporate support, and scale-ability.|LightGBM has narrow user base as it is relatively new<br><br><br>[Go To Top](#Contents)|


[Go To Top](#Contents)

# <code style="background:yellow;color:black">Classification</code>					
"Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data.  The process starts with predicting the class of given data points. The classes are often referred to as target, label or categories.  The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables.  The main goal is to identify which class/category the new data will fall into.
Some classification algorithms are listed in the table below"

|**ALGORITHMS**|**DESCRIPTION**|**HYPER PARAMETER**|**PROS**|**CONS**|
|:-:|:-|:-|:-|:-|
|<a id='21' >**Logistic Regression**|In this algorithm, the probabilities describing the possible outcomes of a single trial are modelled using a logistic function. We can use logistic regression only when the dependent variable is binary (0/ 1, True/ False, Yes/ No) in nature|**Penalty** : It is a regularization technique.Default = ‘L2’.Other supported techniques are:‘l1’, ‘l2’, ‘elasticnet’, ‘none’.<br>**C**: Default = 1.0.Regularization parameter. The strength of the   regularization is inversely proportional to C. Must be strictly positive. <br>**Solver**: Default=’lbfgs’.Algorithm to use in the optimization problem other supported techniques {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}.For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’|Logistic Regression is most useful for understanding the influence of several independent variables on a single outcome variable|•  Assumes all predictors are independent of each other|
|<a id='22' >**Naïve Bayes**|It is based on Bayes’s theorem which gives an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Naive Bayes classifier performs better on less training data compare to other models like logistic regression. It cannot be used when If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction.|***It doesn’t have hyperparameter as such but depending upon the distribution of data below implementations can be applied.***<br>•  **GaussianNB**: The likelihood of the features is of Gaussian  Distribution.<br>•  **MultinomialNB**: For multinomially distributed data.<br>•  ComplementNB: Is best suited for Imbalanced Datasets.<br>•  **BernoulliNB**: For multivariate Bernoulli distribution data•  CategoricalNB: For categorical distributed data.<br>•  **Partial_fit**:   This method can be used when full training data might not fit in memory.|•  Requires a small amount of training data to estimate the necessary parameters to get the results<br>•  They are extremely fast in nature compared to other classifiers|Cannot incorporate feature interactions<br><br><br>[Go To Top](#Contents)|
|<a id='23' >**Support Vector Machine**|The support vector machine is a classifier that represents the training data as points in space separated into categories by a gap as wide as possible. New points are then added to space by predicting which category they fall into and which space they will belong to|**Loss**; default = ‘squared_hinge’ measures how well the network predicts outputs on the test set. Other loss fuctions are: ‘log’, ‘modified_huber’, ‘hinge’, ‘perceptron' ; which log fuction to use is very case specific<br>**Penalty**; default ‘l2’  it is a regularization technique other supported techniques are:‘l1’, ‘l2’, ‘elasticnet’, ‘none’ By changing the regularization technique we can get the important features that affect the classification <br>**Alpha**; default =0.001 It is learning rate for penality. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom|•  It uses a subset of training points in the decision function which makes it memory efficient|•  It is highly effective in high dimensional spaces|The only disadvantage with the support vector machine is that the algorithm does not directly provide probability estimates|
|<a id='24' >**Stochastic Gradient Descent**|Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately|**Loss**; default = ‘hinge’ measures how well the network predicts outputs on the test set, other loss fuctions are: ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron; which log fuction to use is very case specific<br>**Penalty**; default ‘l2’   it is a regularization technique, Other supported techniques are:‘l1’, ‘l2’, ‘elasticnet’, ‘none’. By changing the regularization technique we can get the important features that affect the classification <br>**Alpha**; default =0.001 It is learning rate for penality. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom|•  It is particularly useful when the sample data is in a large number<br>•  Ease of implementation<br>•  It supports different loss functions and penalties for classification|•  It requires a number of hyper-parameters<br>•  It is sensitive to feature scaling<br><br><br>[Go To Top](#Contents)|
|<a id='25' >**K-Nearest Neighbor **|It is a lazy learning algorithm that stores all instances corresponding to training data in n-dimensional space. To label a new point, it looks at the labeled points closest to that new point also known as its nearest neighbors. We should use KNN when the dataset is small and speed is a priority (real-time).|**n_neighbors**; default=5 it is the number of neighbors to use for kneighbors queries<br> **algorithm**; default='auto' other algorithms for making trees are:‘ball_tree’, ‘kd_tree’, ‘brute’<br>**metric**; default='minkowski' measue of distance between the data points. It can be changed according to data. The supported distance metrics are: 'manhattan', 'euclidean'|•  This algorithm is quite simple in its implementation<br>•New data can be added seamlessly|•  Computation cost is pretty high compared to other algorithms<br>•  Does not work well with high dimensions<br>•  Sensitive to outliers and missing values|
|<a id='26' >**Decision Tree**|The decision tree algorithm builds the classification model in the form of a tree structure. It utilizes the if-then rules which are equally exhaustive and mutually exclusive in classification. We can use decision tree when there are missing values in the data and when pre processing time is to be reduced as it does not require pre processing|**Criterion**; default= ""gini"" measures the quality of split; Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. We can use either; the only difference is Entropy might be a little slower than Gini<br>**max_depth**; default=None The maximum depth of the tree. How to find the best value:  Higher  max depth might result in overfitting and vice versa. We can let the model decide the max_depth first and then finalize by comparing the train and test scores<br> **min_samples_split**; default=2 The minimum number of samples required to split an internal node. Higher values generally prevens over-fitting. Depending on the level of underfitting or overfitting, we can tune the values for min_samples_split.<br>**min_samples_leaf**; default=1 The minimum number of samples required to be at a leaf node min_samples_leaf is also used to control over-fitting by defining that each leaf has more than one element.<br> **max_features**; default=None The number of features to consider when looking for the best splits. A reduced number of features we can increase the stability of the tree and reduce variance.|•  Compared to other algorithms decision trees requires less effort for data preparation during  pre-processing<br>•  A decision tree does not require normalization of data<br>•  Missing values in the data also does NOT affect the process of building decision tree to any considerable extent<br> •  A Decision trees model is very intuitive and easy to explain to technical teams as well as stakeholders|•  A small change in the data can cause a large change in the structure of the decision tree causing instability<br>•  For a Decision tree sometimes calculation can go far more complex compared to other algorithms<br>•  Decision tree training is relatively expensive as complexity and time taken is more<br>•  Decision Tree algorithm is inadequate for applying regression and predicting continuous values<br><br><br>[Go To Top](#Contents)|
|<a id='27' >**Random Forest**|Random forest are an ensemble learning method. It operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes of the individual trees. A random forest is a meta-estimator that fits a number of trees on various subsamples of data sets and then uses an average to improve the accuracy in the model’s predictive nature. The sub-sample size is always the same as that of the original input size but the samples are often drawn with replacements. We should use this algorithm when we need high accuracy while working with large datasets with higher dimensions. We can also use it if there are missing values in the dataset. We should not use it if we have less time for modeling or if large computational costs and memory space are a constrain.|**n_estimators**; default=100 The number of trees in the forest. The performance of the model sharply increases with increase in its value and then stagnates at a certain level <br>**bootstrap**; default=True Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree<br>**#NOTE**: Rest all the parameters are same as DECISION TREE|•  The predictive performance can compete with the best supervised learning algorithms<br>•  They provide a reliable feature importance estimate<br>•  They offer efficient estimates of the test error without incurring the cost of repeated model<br>•  Training associated with cross-validation|•  An ensemble model is inherently less interpretable than an individual decision tree<br>•  Training a large number of deep trees can have high computational costs (but can be parallelized) and use a lot of memory<br>•  Predictions are slower, which may create challenges for applications |
|<a id='28' >**Extra Tree Classifier**|This is a ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a “forest” to output its classification result. In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest. We should use it when very less variance is required as it shows the least variance as compared to random forest and decision tree.|All parameters are same as RANDOM FOREST. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting|Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance|For large data it would require more time<br><br><br>[Go To Top](#Contents)|
|<a id='29' >**Gradient Boosting Classifier**|Unlike the random forest method that builds and combines a forest of randomly different trees in parallel, where the trees are not independent. Each tree is trained so that it attempts to correct the mistakes of the previous tree in the series. Built in a non-random way, to create a model that makes fewer and fewer mistakes as more trees are added. Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set(Training is sequential in boosting, but the prediction is parallel)|**loss function**; default=’deviance’ It refers to the loss function to be minimized in each split. Generally the default values work fine<br> **learning_rate**;  default=0.1 This determines the impact of each tree on the final outcome. Lower values are generally preferred as they make the model robust to the specific characteristics of tree; however, lower values would take more time for training<br> **subsample**; default=1.0 The fraction of observations to be selected for each tree. Selection is done by random sampling <br>**criterion** default=’friedman_mse’; Supported criteria are ‘friedman_mse’ for the mean squared error with improvement score by Friedman, ‘mse’ for mean squared error, and ‘mae’ for the mean absolute error.<br>**#NOTE**: Rest all the parameters are same as RANDOM FOREST|•  No data pre-processing required <br>•  Often works great with categorical and numerical values as is <br>•  Handles missing data - imputation not required|•  GBMs will continue improving to minimize all errors <br>•  This can overemphasize outliers and cause overfitting<br>•  Must use cross-validation to neutralize|
|<a id='210' >**XGBOOST Classifier**|Rather than training all the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. In a standard ensemble method where models are trained in isolation, all of the models might simply end up making the same mistakes. We should use this algorithm when we require fast and accurate predictions after the model is deployed. We should not use it when training time is less; also since it is a boosting algorithm it is sensitive to outliers.|**learning_rate/eta** default=0.3 <br>**min_split_loss** default=0 <br>**max_depth** default=6<br> **#NOTE**: The default values are changed and rest all parameter remain same as random forest|•  The advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models<br>•  In a standard ensemble method where models are trained in isolation, all of the models might simply end up making the same mistakes|One disadvantage of boosting is that it is sensitive to  outliers since every classifier is obliged to fix the errors in the predecessors [Go To Top](#Contents)|


[Go To Top](#Contents)

# <code style="background:yellow;color:black">Text Classification</code>				
"Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points.The classes are often referred to as target, label or categories.The classification predictive modelling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which class/category the new data will fall into.

Textual Data : Is inherently discrete, sparse and High Dimensional in nature.
Below are the Text classification algorithms which are widely used :

|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='31'></a>**(LINEAR) Support Vector Classification**|The support vector machine is a classifier that represents the training data as points in space separated into categories by a gap as wide as possible. New points are then added to space by predicting which category they fall into and which space they will belong to. More often text classification use cases will have linearly separable data and LinearSVC is apt for such scenarios|**Penalty** : It is a regularization technique. Default = ‘L2’, Other supported techniques are:‘l1’, ‘l2’, ‘elasticnet’, ‘none’<br>**C** : Default = 1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. |•   Linear kernel is good when there is lot of features (High dimensions)<br>•   This class supports both dense and sparse input and the multiclass <br>•   Training with Linear kernel is faster than with any other kernel<br>•   It uses a subset of training points in the decision function which makes it memory efficient|•     Text classification can be challenging when the documents are very small<br>•     Text classification can be challenging when the number of features is larger than the no of samples.|
|<a id='32'></a>**Naïve Bayes**|It is based on Bayes’s theorem which gives an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.|***It doesn’t have hyperparameter as such but depending upon the distribution of data.*** <br>• **MultinomialNB** : For multinomially distributed data.<br>•  **ComplementNB**: Is best suited for Imbalanced Datasets.<br>•  **BernoulliNB**: For multivariate Bernoulli distribution data<br>•  **CategoricalNB**: For categorical distributed data.<br>•  **Partial_fit**:   This method can be used when full training data might not fit in memory.|•  Requires a small amount of training data to estimate the necessary parameters to get the results. <br>•   The independence assumption among features is a strong constraint that yields a highly interpretable model.<br>•  Its performance  increases when there are more classes to predict,so its highly suitable  for Multi Class problem.<br>•  It requires less model training  time which makes them extremely fast in nature compared to other classifiers.<br>•  NB models cannot represent complex behavior so it won’t lead into over fitting.|•	Cannot incorporate feature interactions.<br>•	Its not suitable for dimensions higher than 100K<br><br><br>[Go To Top](#Contents)|
|<a id='33'></a>**Logistic Regression**|In this algorithm, the probabilities describing the possible outcomes of a single trial are modelled using a logistic function. It is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.|**Penalty** : It is a regularization technique Default = ‘L2’, Other supported techniques are:‘l1’, ‘l2’, ‘elasticnet’, ‘none’<br.**C**: Default = 1.0 Regularization parameter. The strength of the   regularization is inversely proportional to C. Must be strictly positive.<br>**Solver**: Default=’lbfgs’. Algorithm to use in the optimization problem, other supported techniques {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}. For small datasets, ‘liblinear’ is a good choice,  whereas ‘sag’ and ‘saga’ are faster for large ones. For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’|•  Logistic Regression is most useful for understanding the influence of several independent variables on a single outcome variable.<br>•  It performs well when the dataset is linearly separable.<br>•  Is less prone to over-fitting but it can overfit in high dimensional datasets(It can be mitigated using Regularization techniques)<br>•  Logistic Regression not only gives a measure of how relevant a predictor (coefficient size) is, but also its direction of association (positive or negative).|•  Main limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. Most of the time data would be a jumbled mess.<br>•  If the number of observations are lesser than the number of features, Logistic Regression should not be used, otherwise it may lead to overfit.<br>•  Logistic Regression can only be used to predict discrete functions. Therefore, the dependent variable of Logistic Regression is restricted to the discrete number set. This restriction itself is problematic, as it is prohibitive to the prediction of continuous data.<br>•  It does not require that the independents be interval and be unbounded.|


[Go To Top](#Contents)

# <code style="background:yellow;color:black">Time Series</code>				
Time series data is characterized by data points taken at uniform interval of time (days,months,years).	Thus it is a sequence of discrete-time data points.	Time series models are very useful models when you have serially correlated data. Most businesses work on time series data to analyze:					
Few examples : Sales numbers for next year,Website Traffic,Budgetary Analysis,Demand of Products						
There can be two goals depending on whether we want to understand the underlying dataset or making predictions. Basically, we can carry out of the following procedures while working with time series datasets.<br>					
▻ Time Series Analysis - In descriptive modeling, or time series analysis, a time series is modeled to determine its components in terms of seasonal patterns, trends, relation to external factors, and the like<br>					
▻ Time Seires Forecasting - Time series forecasting uses the information in a time series (perhaps with additional information) to forecast future values of that series	<br>				
Note : The procedure for time series analysis can be an optional step while going for forecasting problems.					
						
Time Series Forecasting Algorithms					
						
|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='41'></a>**Autoregression (AR)**|The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps. The method is suitable for univariate time series without trend and seasonal components.|**endog**- The independent variable<br>**missing** -  Used to process the missing values.|• It is simple to apply and understand<br>• Unlike linear regression it considers multiple lag variable|• It assumes that the the previous step is useful in predicting the next time step, which may not be the case always.<br>• It assumes that the data is stationary.<br><br>[Go To Top](#Contents)|
|<a id='42'></a>**Moving Average (MA)**|The moving average (MA) method models the next step in the sequence as a linear function of the residual errors from a mean process at prior time steps.The method is suitable for univariate time series without trend and seasonal components.|**endog**- The independent variable<br>**order** -  The (p,q) order of the model for the number of AR parameters, and MA parameters to use.<br>Note : In case of moving average, the AR parameter will be zero.<br> p - AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1).x(t-5).<br> q - MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1).e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.|• Moving averages can be used for measuring the trend of any series. This method is applicable to linear as well as non-linear trends.<br>•  It is simple to apply and understand<br>• A moving average can actually be quite effective, especially if you pick the right p for the series.|• The trend obtained by moving averages generally is neither a straight line nor a standard curve. For this reason the trend cannot be extended for forecasting future values.<br>• Trend values are not available for some periods at the start and some values at the end of the time series.<br><br>[Go To Top](#Contents)|
|<a id='43'></a>**Auto Regressive Integrated Moving Average (ARIMA)**|The Autoregressive Integrated Moving Average (ARIMA) method models the next step in the sequence as a linear function of the differenced observations and residual errors at prior time steps. It combines both Autoregression (AR) and Moving Average (MA) models as well as a differencing pre-processing step of the sequence to make the sequence stationary, called integration (I). The method is suitable for univariate time series with trend and without seasonal components.|**endog**- The independent variable<br>**order** -  The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use.<br> p - AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1).x(t-5).<br> q - MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1).e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. d - These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.|• Unlike regression, ARIMA (in its basic form) does not require a set of predictor variable. Only the target variable does the job<br>•  ARIMA if fitted well can forecast small ups and downs much better than regression, but given that the overall trend has not changed.<br>•  ARIMA forecasts are usually more accurate and reliable.|• No explicit seasonal indices<br>• Hard to interpret coefficients or explain “how the modelworks”<br>• Danger of overfitting or mis-identification if not used with care.<br>•  Cannot model seasonal data<br><br>[Go To Top](#Contents)|
|<a id='44'></a>**Seasonal Autoregressive Integrated Moving-Average (SARIMA)**|The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence as a linear function of the differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps. It combines the ARIMA model with the ability to perform the same autoregression, differencing, and moving average modeling at the seasonal level. The method is suitable for univariate time series with trend and/or seasonal components.|**endog**- The independent variable<br>**order** -  The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use. <br>p - AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1).x(t-5). <br>q - MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1) .e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. d - These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.<br>**seasonal_order** - <br>P: Seasonal autoregressive order. <br>D: Seasonal difference order. <br>Q: Seasonal moving average order. m: The number of time steps for a single seasonal period.|•  Unlike regression, SARIMA (in its basic form) does not require a set of predictor variable. Only the target variable does the job<br>• SARIMA if fitted well can forecast small ups and downs much better than regression, but given that the overall trend has not changed.<br>• SARIMA forecasts are usually more accurate and reliable.<br>• SARIMA has the ability to model seasonal data.|• Hard to interpret coefficients or explain “how the modelworks”<br>• Danger of overfitting or mis-identification if not used with care<br><br>[Go To Top](#Contents)|
|<a id='45'></a>**Vector Autoregression (VAR)**|The Vector Autoregression (VAR) method models the next step in each time series using an AR model. It is the generalization of AR to multiple parallel time series, e.g. multivariate time series. The method is suitable for multivariate time series without trend and seasonal components.|**endog**- The independent variable<br>**exog** - The independent variable.<br>**dates** - Must match number of rows of endog|• It is simple to apply and understand<br>• Unlike linear regression it considers multiple lag variable<br>• It can handle multiple variable time series data|• The impulse-response functions will depend on the ordering of the variables.<br>• It does not have a clear ad hoc specification and doesn't give us a clear glimpse of the underlying relation among the variables.<br><br>[Go To Top](#Contents)|
|<a id='46'></a>**ARIMAX/SARIMAX**|An Autoregressive Integrated Moving Average with Explanatory Variable (ARIMAX) model can be viewed as a multiple regression model with one or more autoregressive (AR) terms and/or one or more moving average (MA) terms. This method is suitable for forecasting when data is stationary/non stationary, and multivariate with any type of data pattern.|**exog** -  The independent variable<br>**order** -  The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use. <br>p - AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1).x(t-5). <br>q - MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1) .e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. d - These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.<br>**for SARIMAX**<br>**seasonal_order** - <br>P: Seasonal autoregressive order. <br>D: Seasonal difference order. <br>Q: Seasonal moving average order. m: The number of time steps for a single seasonal period.|The X added to the end stands for “exogenous”. In other words, it suggests adding a separate different outside variable to help measure our endogenous variable which lacks in ARIMA.<br>• SARIMAX has the ability to work on datasets with missing values.|The covariate coefficient is hard to interpret.<br><br>[Go To Top](#Contents)|





[Go To Top](#Contents)


```python

```
