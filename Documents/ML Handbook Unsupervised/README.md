## Contents

|[Anomaly Detection](#Anomaly-Detection)|[Association Rules](#Association-Rules)|[Bayesian Network](#Bayesian-Network)|[Clustering](#Clustering)|[Dimensionality Reduction Techniques](#Dimensionality-Reduction-Techniques)|[Recommender System](#Recommender-System)
|:-|:-|:-|:-|:-|:-|
|• [One-Class SVM](#12)<br> • [Isolation Forests](#13)<br> • [Elliptic Envelope](#14)<br> • [DBSCAN](#15)<br> • [PCA Based Anomaly Detection](#16)<br> • [Local Outlier Factor](#17)<br> • [Feature Bagging](#18)<br> • [KNN](#19)<br> • [HBOS](#20)<br> • [CBLOF](#21)<br>|• [Apriori Algorithm](#112)<br>• [FP Growth Algorithm](#113)<br> |• [Bayesian Time Series](#212)<br>• [Bayesian Belief Network](#213)<br>|• [Affinity Propagation](#312)<br>• [Agglomerative Clustering](#313)<br>• [BIRCH](#314)<br>• [DBSCAN](#315)<br>• [K-Means](#316)<br>• [Mini-Batch K-Means](#317)<br>• [Mean Shift](#318)<br>• [OPTICS](#319)<br>• [Spectral Clustering](#320)<br>• [Gaussian Mixture Model](#321)<br>• [K- Mode](#322)<br>• [K-Prototypes](#323)<br>|• [PCA](#412)<br>• [LDA](#413)<br>• [SVD](#414)<br>• [PLSR](#415)<br>• [t-SNE](#416)<br>• [Factor Analysis](#417)<br>• [Isomap](#418)<br>|• [Content based filtering](#512)<br>• [Collaborative filtering](#513)<br>• [Item based Collaborative filtering](#514)<br>• [User based Collaborative filtering](#515)<br>• [Hybrid recommender system](#516)<br>

[Go To Top](#Contents)

# <code style="background:yellow;color:black">Anomaly Detection</code>					
**What is Anomaly?**
Anomalies are defined as events that deviate from the standard, happen rarely, and do not follow the rest of the “pattern”. Anomalies are also referred to as outliers, surprise, aberrant, deviation, peculiarity, etc.<br>
**Real-time Applications:** 
Intrusion detection, fraud detection, system health monitoring, event detection in sensor networks, and detecting eco-system disturbances<br>
**Challenges in Anomaly Detection:**<br>
•	The difficulty to achieve high anomaly detection recall rate<br>
•	Anomaly detection in high-dimensional space - Anomalies often exhibit evident abnormal characteristics in a low-dimensional space yet become hidden and unnoticeable in a high-dimensional space<br>
•	Due to the difficulty and cost of collecting large-scale labelled anomaly data, it is important to have data-efficient learning of normality/abnormality<br>
**Unsupervised methods are the best choice for anomaly detection, since they recognize new and unknown objects whereas supervised methods can detect only pre-known abnormal cases. 
|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='12' ></a>**One-Class SVM**|A One-Class Support Vector Machine is an unsupervised learning algorithm for novelty detection and can be used for time series data as well.It learns the boundaries of these points and is therefore able to classify any points that lie outside the boundary i.e. outliers.Novelty detection - Classifying new data even it was not captured in training data|**• Kernel:** [{'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf']It maps the observations into some feature space. Ideally the observations are more easily (linearly) separable after this transformation.<br>**• Gamma:** [ {'scale', 'auto'} or float, default='scale' ]Kernel co-efficient/Parameter of gaussian distribution (determines the smoothing of the contour lines)<br>**• nu:** [float, default=0.5] It is same as the contamination hyperparameter in other algorithms (proportion of outliers you expect to observe)<br>|•  SVM is efficient for novelty detection <br>•  Less computation time and it is relatively memory efficient|•  SVM algorithm is not suitable for large data sets<br>•  No probabilistic explanation for the classification since SVM works by putting data points above and below the hyper plane<br>• Data comes in a stream and dealing with the data in real-time is quite tedious
|<a id='13' ></a>**Isolation Forests**|Isolation Forest is unsupervised algorithm and it is based on the Decision Tree algorithm.It identifies anomaly by isolating outliers in the data. This random partitioning of features will roduce shorter paths in trees for the anomalous data points, thus distinguishing them from the rest of the dataIsolation Forest uses an ensemble of Isolation Trees for the given data points to isolate anomalies|**• n_estimators:** [int, default=100]The number of trees to use<br>**• max_samples:** ["auto", int or float, default="auto"]The number of samples to draw while build a single tree.<br>**• contamination:** ['auto' or float, default='auto']Parameter is chosen based on the proportion of outliers in the data<br>**• Max features:** [int or float, default=1.0] It is the number of features to draw from the total features to train each base estimator or tree.|• Low linear time complexity and a small memory requirement<br>• Works well with small dataset.<br>• Isolation Forest shows enviable performances when working with high-dimensional or redundant data<br>|• Isolation Forest is not suitable for multi-variate time series
|<a id='14' ></a>**Elliptic Envelope**|The Elliptic Envelope method fits a multivariate gaussian distribution to the dataset.It assumes the data is normally distributed and based on that assumption “draws” an ellipse around the data, classify data point as outlier if it is outside the ellipse otherwise as inlier|**• Contamination:** [float, default=0.1, Range is (0, 0.5)]Parameter is chosen based on the proportion of outliers in the data<br>**• Support Fraction:** [float, default=None, Range is (0, 1) ]The proportion of points to be included in the support of the raw MCD estimate|• Model is efficient when data is normally distributed|• Model does not perform well in high dimensional spaces<br>• Model does not perform well when data is not unimodal
|<a id='15' ></a>**DBSCAN**|DBSCAN algorithm groups together the points that are closely packed together and marks the low density points far apart as outliers.It’s like K-means, except the number of clusters does not need to be specified in advance.|**• min_samples:** [int, default=5]minimum number of data points to define a cluster<br>**• eps:** [float, default=0.5]Distance that specifies the neighborhoods.<br>**• metric:** [{precomputed or euclidean}, default='euclidean'] Calculating distance between instances in a feature array.|• Model is best choice for time series anomaly detection|• Model does not perform well in high dimensional spaces
|<a id='16' ></a>**PCA Based Anomaly Detection**|PCA-Based Anomaly Detection uses distance metrics to differentiate between normal and anomalous behavior.<br>Dimensionality reduction algorithm try to capture the most salient information of the original features to minimise the reconstruction error.<br>The algorithms will have the largest reconstruction error on those data points that are hardest to model and those occur the least often and are the most anomalous. Since outliers are rare should exhibit the largest reconstruction error|**• n_components:** [int, float, None or str]Number of components to keep in the modelling|• Performs well with very large datasets and high dimensionality|• May lead to a significant loss of information
|<a id='17' ></a>**Local Outlier Factor**|LOF uses density-based outlier detection to identify local outliers, points that are outliers with respect to their local neighborhood, rather than with respect to the global data distribution.<br>The higher the LOF value for an observation, the more anomalous the observation.<br>A point is labeled as an outlier if the density around that point is significantly different from the density around its neighbors.|**• n_neighbors:** [int, default=20]Number of neighbors to use by default for kneighbors queries.<br>**• Contamination:**['auto' or float, default='auto']Parameter is chosen based on the proportion of outliers in the data|• Model performs well with high-dimensional data|• Local density estimate of LOF is not accurate enough to detect outliers in the complex and large databases<br>• Unsure about determining right size of the k-nearest neighbours and threshold value
|<a id='18' ></a>**Feature Bagging**|Feature bagging approach combines results from multiple outlier detection algorithms that are applied using different set of features. <br>A feature bagging detector fits a number of base detectors on various sub-samples of the dataset.<br>It uses averaging or other combination methods to improve the prediction accuracy. This brings out the diversity of base estimators.|**• Base estimator:** LOF, KNN, ABOD could be used as the base estimator<br>**• n_neighbors:** [int, optional (default=10)] Number of neighbors to use by default for kneighbors queries.<br>**• Contamination:** [float in (0., 0.5), optional (default=0.1)]Parameter is chosen based on the proportion of outliers in the data|• Performs well with very large, high dimensional and noisy databases |• Unsure about determining right size of the k-nearest neighbours and threshold value
|<a id='19' ></a>**KNN**|KNN algorithm identified anomalies based on three approaches:<br>Largest: Uses the distance of the kth neighbor as the outlier score<br>Mean: Uses the average of all k neighbors as the outlier score<br>Median: Uses the median of the distance to k neighbors as the outlier score|**• Method:** [(default='largest') {'largest', 'mean', 'median'}]Uses Mean, Median or Largest distance of the k-neighbor as outlier score|• Very simple algorithm and easy to implement<br>• Works efficiently with small amount of data|• Does not work well with large data sets<br>• Does not work well with high dimensionality
|<a id='20' ></a>**HBOS**|HBOS is histogram-based anomaly detection algorithm.<br>HBOS assumes the feature independence and calculates the degree of anomalies by building histograms.<br>In multivariate anomaly detection, a histogram for each single feature can be computed, scored individually and combined at the end.<br>Usecase: Best choice for anomaly detecction in computer networks due to its low computational time|**• contamination:** [float in (0., 0.5), (default=0.1)]Parameter is chosen based on the proportion of outliers in the data<br>**• n_bins:** [int, (default=10)]The number of bins.|• Less computation time compare to clustering-based algorithms and nearest-neighbor based methods|• Not efficient with high dimensional data<br>• Performs poor on local outlier problems
|<a id='21' ></a>**CBLOF**|The CBLOF calculates the outlier score based on cluster-based local outlier factor.<br>An anomaly score is computed by the distance of each instance to its cluster center multiplied by the instances belonging to its cluster. |**• contamination:** [float in (0., 0.5), (default=0.1)] Parameter is chosen based on the proportion of outliers in the data <br>**• n_clusters:** [int, (default=8)]|• Easily adaptable to on-line / incremental mode suitable for anomaly detection from temporal data.|• Model is computationally very complex.<br>• If data points do not create any clusters the technique may fail



[Go To Top](#Contents)

# <code style="background:yellow;color:black">Association Rules</code>				
**What is Association rule?**
Association rule mining is a procedure which aims to observe frequently occurring patterns, correlations, or associations from datasets found in various kinds of databases such as relational databases, transactional databases, and other forms of repositories. 

Association rules are created by thoroughly analyzing data and looking for frequent if/then patterns. Then, depending on the following parameters, the important relationships are observed:
<br>
•   **Support:** Support indicates how frequently the if/then relationship appears in the database.<br>
•   **Confidence:** Confidence tells about the number of times these relationships have been found to be true.<br>
•   **Lift:** Lift is nothing but the ratio of confidence to expected confidence. Lift is a value that gives us information about the increase in probability of the "then" (consequent) given the "if" (antecedent) part.<br>

**Real-time Applications:** 
Market basket analysis, Medical diagnosis, Protein sequences,  Census data, CRM of credit card business etc.

|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='112' ></a>**Apriori Algorithm**|Apriori algorithm finds the most frequent itemsets or elements in a transaction database and identifies association rules between the items.<br>The algorithm uses a “bottom-up” approach, where frequent subsets are extended one item at once (candidate generation) and groups of candidates are tested against the data.|**• min_support:**  {float, (default: 0.5)} A float between 0 and 1 for minumum support of the itemsets returned.<br>**• use_colnames:** {bool (default: False)} If True, uses the DataFrames' column names in the returned DataFrame instead of column indices.<br>**• max_len:** {int (default: None)} Maximum length of the itemsets generated.<br>**• verbose:** {int (default: 0)}Shows the number of iterations if >= 1 and low_memory is True. If=1 and low_memory is False, shows the number of combinations.<br>**• low_memory:** {bool (default: False)}If True, uses an iterator to search for combinations above min_support. Should only be used for large dataset  if memory resources are limited.<br>|• This is the most simple and easy-to-understand algorithm among association rule learning algorithms<br>• The resulting rules are intuitive and easy to communicate to an end user<br>• The algorithm is exhaustive, so it finds all the rules with the specified support and confidence.|• If the dataset is small, the algorithm can find many false associations that happened simply by chance (Transaction frequency are less).<br>• The algorithm doesn't take into account hierarchies of products or quantities of the items bought in a transaction.<br>• The algorithm is computationally expensive.
|<a id='113' ></a>**FP Growth Algorithm**|FP-Growth is an algorithm for extracting frequent itemsets with applications in association rule learning that emerged as a popular alternative to the established Apriori algorighm.<br>It does not require candidate generation. Internally, it uses a so-called FP-tree (frequent pattern tree) datastrucure without generating the candidate sets explicitely, which makes it particularly attractive for large datasets.<br>This can be used to identify commonly occurring items that can be used to make decisions, suggest items, make forecasts, and so on.|**• min_support:** {float, (default: 0.5)}A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions.<br>**• use_colnames:** {bool (default: False)} If True, uses the DataFrames' column names in the returned DataFrame instead of column indices.<br> **• max_len:** {int (default: None)} Maximum length of the itemsets generated.<br> **• verbose:** {int (default: 0)} Shows the stages of conditional tree generation.|• The FP-growth algorithm scans the dataset only twice. <br>• Usually faster than Apriori. |•  FP Tree is more cumbersome and difficult to build than Apriori.<br>• The FP-Growth algorithm may not fit into the memory.<br>•  Dataset with infrequent transactions degrade algorithm performance.

[Go To Top](#Contents)

# <code style="background:yellow;color:black">Bayesian Network</code>				

Bayesian networks are a type of probabilistic graphical model that uses Bayesian inference for probability computations. 

Bayesian networks aim to model conditional dependence, and therefore causation, by representing conditional dependence by edges in a directed graph. Through these relationships, one can efficiently conduct inference on the random variables in the graph through the use of factors.

**Bayesian Networks Application**
Bayesian Networks have innumerable applications in a varied range of fields including healthcare, medicine, bioinformatics, information retrieval and so on. Here’s a list of real-world applications of the Bayesian Network:

1. **Disease Diagnosis:** Bayesian Networks are commonly used in the field of medicine for the detection and prevention of diseases. They can be used to model the possible symptoms and predict whether or not a person is diseased.
2. **Optimized Web Search:** Bayesian Networks are used to improve search accuracy by understanding the intent of a search and providing the most relevant search results. They can effectively map users intent to the relevant content and deliver the search results.
3. **Spam Filtering:** Bayesian models have been used in the Gmail spam filtering algorithm for years now. They can effectively classify documents by understanding the contextual meaning of a mail. They are also used in other document classification applications.
4. **Gene Regulatory Networks:** GRNs are a network of genes that are comprised of many DNA segments. They are effectively used to communicate with other segments of a cell either directly or indirectly. Mathematical models such as Bayesian Networks are used to model such cell behavior in order to form predictions.
5. **Biomonitoring:** Bayesian Networks play an important role in monitoring the quantity of chemical dozes used in pharmaceutical drugs.

|ALGORITHMS|DESCRIPTION|PROS|CONS|
|:-|:-|:-|:-|
|<a id='212' ></a>**Bayesian Time Series**|Bayesian Structural Time Series model is also known as ‘state space models’ and ‘dynamic linear models’ is a class of time series model which can fit the structural change in time series dynamically. <br>This technique is more transparent than ARIMA models and deals with uncertainty in a more elegant manner. <br>BSTS is the implementation of this model in R which is easy to use a function which requires a minimal mathematical understanding of the state space models.|• Forecasting with few data points<br>• Time variant co-efficient <br>• Handling non stationary time series<br>• Handling the high variablility in data<br>•Smoothing parameter estimation from the data  |• Higher run time <br>• low interperitbility <br>• High code complexbility 
|<a id='213' ></a>**Bayesian Belief Network**|Belief Network is a Probabilistic Graphical Model (PGM) that represents conditional dependencies between random variables through a Directed Acyclic Graph (DAG).<br>A DAG models the uncertainty of an event occurring based on the Conditional Probability Distribution (CDP) of each random variable. A Conditional Probability Table (CPT) is used to represent the CPD of each variable in the network.|•Can calculate explicit probabilities for hypotheses.<br>•Can be used for statistically-based learning.|•Require initial knowledge of many probabilities.<br>• Tedious to build conditional probability table<br>• Can become computationally intractable.


[Go To Top](#Contents)


# <code style="background:yellow;color:black">Clustering</code>		
Clustering or cluster analysis is an unsupervised learning problem. Sometimes, rather than ‘making predictions’, we instead want to categorize data into buckets. This is termed “unsupervised learning.”
Clustering techniques apply when there is no class to be predicted but rather when the instances are to be divided into natural groups.
A cluster is often an area of density in the feature space where examples from the domain (observations or rows of data) are closer to the cluster than other clusters. The cluster may have a center (the centroid) that is a sample or a point feature space and may have a boundary or extent. These clusters presumably reflect some mechanism at work in the domain from which instances are drawn, a mechanism that causes some instances to bear a stronger resemblance to each other than they do to the remaining instances.
It is often used as a data analysis technique for discovering interesting patterns in data.
There are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of clustering algorithms and different configurations for each algorithm.
|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='312' ></a>**Affinity Propagation**|It is an unsupervised ML algorithm that is particularly well suited for problems where we don’t know the optimal number of clusters. Real-valued messages are exchanged between data points until a high-quality set of exemplars (appropriate model / prototypes) and corresponding clusters gradually emerges. exchanging messages between points’ is the same thing as manipulating matrices, for values of Similarit) and Preferences (point’s suitability to be an exemplar)Each iteration has two message-passing steps: 1. Calculating responsibilities: How well-suited point k is to serve as the exemplar for point i.2.  Calculating availabilities: How appropriate it would be for point i to choose point k as its exemplar.|**• preference:**  controls how many exemplars are chosen. (float, default=None)<br>**• damping factor:** (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values in order to avoid numerical oscillations when updating these values (messages).<br>**• affinity:** {​​​​​​​'euclidean', 'precomputed'}​​​​​​​, (default='euclidean') .Which affinity to use.|•  Don’t have to explicitly specify the number of clusters.<br>• Affinity Propagation is most appropriate for small to medium sized datasets.|•  The main drawback of Affinity Propagation is its computational and memory complexity<br?• .It is hard to know the value of preferences which can yield an optimal clustering solution.<br>• .When oscillations occur, AP cannot automatically eliminate them.
|<a id='313' ></a>**Agglomerative Clustering**|Agglomerative clustering is a method of cluster analysis to build a hierarchy of clusters. This is a bottom-up approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Apart from distance functions,linkage (distance) criterion is also used, such as single-linkage (the minimum of object distances), complete-linkage (the maximum of object distances), average-linkage (Arithmetic Mean') or ward (minimizes the variance of the clusters being merged)|**• n_clusters:** {​​​​​​​int or None, default=2}​​​​​​​ an estimate of the number of clusters in the data<br>**• affinity:** {​​​​​​​default='euclidean'}​​​​​​​Metric used to compute the linkage.<br>**• linkage:** {​​​​​​​default="ward"}​​​​​​​ The linkage criterion determines which distance to use between sets of observation.|• Do not need to specify the number of clusters<br>• Single-link algorithms are best for capturing clusters of different sizes and shapes<br>• Complete link and group average are not affected by noise<br>• It can produce an ordering of the objects, which may be informative for data display.|•  Single linkage alogrithms are sensitive to noise<br>•  Time complexity for the clustering can result in very long computation on larger datasets.<br>• Complete link and group average have a bias towards finding global patterns<br>• No provision can be made for a relocation of objects that may have been 'incorrectly' grouped at an early stage.
|<a id='314' ></a>**BIRCH**|BIRCH stands for Balanced Iterative Reducing and Clustering Using Hierarchies, which uses hierarchical methods to cluster and reduce data. Uses a tree structure to create a cluster called the Clustering Feature Tree (CF Tree). Each node  is composed of several Clustering features (CF). BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resourcesIt is a memory-efficient algorithm provided as an alternative to MiniBatchKMeans.|**• threshold:** {​​​​​​​float, default=0.5}​​​​​​​ limits the distance between the entering sample and the existing subclusters<br>**• branching factor:** {​​​​​​​int, default=50}​​​​​​​limits the number of CF subclusters in each node<br>**• n_clusters:** {​​​​​​​int, default=3}​​​​​​​estimated number of clusters in the data.|•  BIRCH attempts to minimize the memory requirements of large datasets by summarizing the information contained in dense regions as Clustering Feature (CF).<br>•  The clustering speed is fast, and it only takes  one scan of the training set to build the CF Tree.<br>•  BIRCH can also do some additional outlier detection and preliminary data pre-processing according to category specifications.<br>•  Other clustering algorithms can be applied to the subclusters produced by BIRCH.|•  Birch does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than twenty, it is generally better to use other algorithms like Mini Batch K-Means<br>• Parameter adjustment in BIRCH is more complicated than K-Means and Mini Batch K-Means, because it needs to adjust several key parameters of CF Tree.
|<a id='315' ></a>**DBSCAN**|The DBSCAN algorithm views clusters as areas core samples of high density separated by areas of low density. Clusters found by DBSCAN can be of any shape, as opposed to k-means which assumes that clusters are convex shaped. When the data set is dense and is not convex, then DBSCAN will be much better than K-Means clustering.|**• min_samples:** {​​​​​​​int, default=5}​​​​​​​ minimum number of data points to define a cluster<br>**• eps:** {​​​​​​​float, default=0.5}​​​​​​​ Distance that specifies the neighborhoods. Two points are considered to be neighbors if the distance between them are less than or equal to eps. Higher min_samples or lower eps indicate higher density necessary to form a cluster.|•  Does not require to specify number of clusters beforehand<br>•  Performs well with arbitrary shapes clusters<br>•  DBSCAN is robust to outliers and able to detect the outliers|•  In some cases, determining an appropriate distance of neighborhood (eps) is not easy and it requires domain knowledge<br>•  If the density is not uniform and the difference in cluster spacing is very different, the cluster quality is poor, then DBSCAN clustering is generally not suitable.<br>• DBSCAN is usually suitable for cluster analysis of lower-dimensional data<br> • If the sample set is large, the clustering convergence time is long.
|<a id='316' ></a>**K-means**|K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid. The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.|**• n_clusters:** int, default=8 Estimated number of clusters in the data. <br>**• Init:** k-means++, random, ndarray, callable   default= k-means++ Method for initialization|• Relatively simple to implement.<br>• Scales to large data sets.<br>• Easily adapts to new examples.<br>• Generalizes to clusters of different shapes and sizes, such as elliptical clusters.<br>• Can warm-start the positions of centroids.|• K-means doesn’t allow development of an optimal set of clusters and for effective results, you need to decide on the number of clusters before.<br>• K-means clustering gives varying results on different runs of an algorithm.<br>• K Means is sensitive to outliers as Centroids can be dragged by outliers.
|<a id='317' ></a>**Mini-Batch K-Means**| Mini-Batch K-Means is a modified version of k-means that makes update to the cluster centroids using mini-batches of samples rather than the entire dataset.<br>Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence.|**• n_clusters:** int, default=8 Estimated number of clusters in the data. <br>**• Init:** k-means++, random, ndarray, callable   default= k-means++ Method for initialization|• Faster for large datasets as it saves computational time.<br>• More robust to statistical noise than K-Means Clustering|• MiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced
|<a id='318' ></a>**Mean Shift**|Mean Shift is a hierarchical clustering algorithm. Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. Mean-shift algorithm has applications in the field of image processing and computer vision |**• bandwidth:** float, default=None the interaction between the length <br>**• Seeds:** array-like of shape (n_samples, n_features), default=None Seeds used to initialize kernels|•  The output of the algorithm is independent of initializations. <br>•  Model-free, doesn’t assume any prior shape like spherical, elliptical, etc. on data clusters. <br> •  Robust to outliers. |•  Computationally expensive. <br> •  Doesn’t scale well with dimension of feature space.
|<a id='319' ></a>**OPTICS**|OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above. The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. It adds two more terms to the concepts of DBSCAN clustering: 1. Core Distance: It is the minimum value of radius required to classify a given point as a core point. 2. Reachability Distance: It is defined with respect to another data point q. The Reachability distance between a point p and q is the maximum of the Core Distance of p and the Euclidean Distance(or some other distance metric) between p and q. This technique does not explicitly segment the data into clusters. Instead, it produces a visualization of Reachability distances and uses this visualization to cluster the data.|**• min_samples:** int > 1 or float between 0 and 1 (default=5) The number of samples in a neighborhood for a point to be considered as a core point <br> **• metric:** str or callable, optional (default=’minkowski’) Metric to use for distance computation.<br>**• eps:** float, (default=None) The maximum distance between two samples for one to be considered as in the neighborhood of the other <br> **• cluster_method:** str, optional (default=’xi’) The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are “xi” and “dbscan”.|• Outlier Detection: An extensionn of OPTICS called OPTICS-OF (OF for Outlier Factor) is used for outlier detection. This will give an outlier score to each point, that is a comparison to its closest neighbors rather than the entire set. <br>• Fewer Parameters: The OPTICS clustering technique does not need to maintain the epsilon parameter and is only given in the above pseudo-code to reduce the time take. This leads to the reduction of the analytical process of parameter tuning.|•  Memory Cost : The OPTICS clustering technique requires more memory as it maintains a priority queue (Min Heap) to determine the next data point which is closest to the point currently being processed in terms of Reachability Distance. It also requires more computational power because the nearest neighbour queries are more complicated than radius queries in DBSCAN <br>•  This technique does not segregate the given data into clusters. It merely produces a Reachability distance plot and it is upon the interpretation of the programmer to cluster the points accordingly.
|<a id='320' ></a>**Spectral Clustering**|Spectral Clustering is a general class of clustering methods, drawn from linear algebra. Spectral clustering is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. Spectral clustering uses information from the eigenvalues (spectrum) of special matrices built from the graph or the data set. It treats each data point as a graph-node and thus transforms the clustering problem into a graph-partitioning problem.|**• n_clusters:** integer, optional The dimension of the projection subspace <br> **• eigen_solver:** {​None, ‘arpack’, ‘lobpcg’, or ‘amg’}​ <br> The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. <br>**• n_components:** {​integer, optional}​ default=n_clusters Number of eigen vectors to use for the spectral embedding|•  Clusters not assumed to be any certain shape/distribution, in contrast to e.g. k-means. This means the spectral clustering algorithm can perform well with a wide variety of shapes of data <br> • It can correctly cluster observations that actually belong to the same cluster but are farther off than observations in other clusters due to dimension reduction. <br> • Reasonably fast for sparse data sets of several thousand elements.|• Need to choose the number of clusters k. <br> • Use of K-Means clustering in the final step implies that the clusters are not always the same. They may vary depending on the choice of initial centroids. <br> • Computationally expensive for large datasets. This is because eigenvalues and eigenvectors need to be computed and then we have to do clustering on these vectors
|<a id='321' ></a>**Gaussian Mixture Model**|Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. <br>Mixture models in general don't require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically.<br>The Gaussian mixture model attempts to find a mixed representation of the probability distribution of the multidimensional Gaussian model, thereby fitting a data distribution of arbitrary shape.<br>GMMs have been used for feature extraction from speech data, and have also been used extensively in object tracking of multiple objects, where the number of mixture components and their means predict object locations at each frame in a video sequence. |**• n_components:** int, default= 1 The number of mixture components <br>**• covariance_type:** {‘tied’, ‘diag’, ‘spherical’} default= 'full' String describing the type of covariance parameters to use <br>**• tol- float:** default= 1e-3 The convergence thereshold <br>**• reg_covar:** float, default= 1e-6. Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive|• It is the fastest algorithm for learning mixture models<br>• As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.<br>• Cluster assignment is much more flexible in GMM than in k-means as each cluster can have unconstrained covariance structure.|• It only guarantees to converge locally.<br>• It converges slowly.
|<a id='322' ></a>**K-Mode**|While K-Means calculates the euclidean distance between two points, K-Modes attempts to minimize a dissimilarity measure and counts the number of feature that are not the same.<br>The k-modes approach modifies the standard k-means process for clustering categorical data by replacing the Euclidean distance function with the simple matching dissimilarity measure, using modes to represent cluster centres and updating modes with the most frequent categorical values in each of iterations of the clustering process.|**• n_clusters:** int, optional, default: 8 Estimated number of clusters in the data. <br>**• Init:** {'Huang', 'Cao', 'random' or a list of ndarrays}, default: 'Cao' Method for initialization|It works well when dataset contains only categorical features.|• A large number of dimensions and huge sets of data items can be complex because of time and space complexity.<br>• In the absence of a distance measure, its value must be defined, which is problematic in multi-dimensional domains
|<a id='323' ></a>**K-Prototypes**|K-Prototypes extends K-Means and K-Modes and is particularly adapted to handle mixed datasets that contain both continuous and categorical variables.<br>To handle both categorical and continuous variables, K-Prototypes uses a custom dissimilarity metric. The distance between a point to its cluster center (its prototype) that is to be minimized by euclidean distance between the continuous variables and C is the count of dissimilar categorical variables<br>K-prototype = k-means for numeric + K-modes for categorical variables|**• n_clusters:** int, optional, default: 8 Estimated number of clusters in the data. <br>**• Init:** {'Huang', 'Cao', 'random' or a list of ndarrays}, default: 'Cao' Method for initialization <br>**• Gamma:** float, default: None Weighing factor that determines relative importance of numerical vs.categorical attributes.|• The algorithm caters scalability, simplicity and speed of convergence. <br>• It handles mixed datasets that contain both continuous and categorical variables.|• The distance metrics for categorical part and numerical part are not defined on the same scale. <br>• To maintain the equality proportions for either type attributes, the user needs to specify a weightage parameter, which is not trivial when no prior knowledge is available, and also the final clustering result is sensitive to this weightage parameter.<br>• It converges not to the global minimum but to a local minimum

[Go To Top](#Contents)

# <code style="background:yellow;color:black">Dimensionality Reduction Techniques</code>		
Dimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space
so that the low-dimensional representation retains some meaningful properties of the original data. Working in 
high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse 
of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields 
that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition,
neuroinformatics, and bioinformatics.

|ALGORITHMS|DESCRIPTION|HYPER PARAMETER|PROS|CONS|
|:-|:-|:-|:-|:-|
|<a id='412' ></a>**PCA**|PCA is a projection technique which find a projection of the data in a smaller dimension. The idea is to find an axis in the data with highest variance and to map the data along that axis.<br>New variables/components derived from PCA are orthogonal and independent of one another.<br>PCA is useful in cases where the dimensions are highly correlated.<br>PCA is an unsupervised ML algorithm.|**• n_components:** int, default=None Number of components to keep. If None, all non-zero components are kept.<br>**• kernel:** Default=”linear”.“linear”, “poly”,“rbf”,“sigmoid”,“cosine”,“precomputed” <br>**• gamma:** float, default=1/n_features Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.<br>**• degree:** int, default=3 Degree for poly kernels. Ignored by other kernels.<br>**• coef0:** float, default=1Independent term in poly and sigmoid kernels. Ignored by other kernels.|• Removes Correlated Features <br>• Improves Algorithm Performance <br>• Reduces Overfitting <br>• Improves Visualization|• Components derived become less interpretable.<br>• Data standardization is must before PCA <br>• Information Loss as it fails to capture all the variance in the dataset <br>• All the categorical features are required to be converted into numerical features before PCA can be applied.
|<a id='413' ></a>**LDA**|Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. <br>It is used for modeling differences in groups i.e. separating two or more classes. It is used to project the features in higher dimension space into a lower dimension space. <br>LDA attempts to create new axes and projects data onto new axes in a way to maximise the separation betweenn the classes.<br>LDA is based on the concept of searching for a linear combination of variables that best separates two classes.|**• solver:**{‘svd’, ‘lsqr’, ‘eigen’}, default=’svd’ ‘svd’: Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features.‘lsqr’: Least squares solution, can be combined with shrinkage.‘eigen’: Eigenvalue decomposition, can be combined with shrinkage.<br>**• shrinkage:** ‘auto’ or float, default=None Shrinkage parameter. Float between 0 and 1: fixed shrinkage parameter.<br>**• n_components:** int, default=None Number of components for dimensionality reduction. This parameter only affects the transform method.|• Linear kernel is preferred when the number of features is quite high (High-dimension dataset) <br>• LDA supports both dense and sparse input and the multiclass <br>• Training with Linear kernel is faster than with any other kernel <br>• It uses a subset of training points in the decision function which makes it memory efficient|• LDA fails when the mean of the distributions are shared. <br>• Requires normalization and a target column to separate the data <br>• All the categorical features are required to be converted into numerical features before PCA can be applied.
|<a id='414' ></a>**SVD**|This is a form of matrix analysis that leads to a low-dimensional representation of a high-dimensional matrix.<br>The decomposition allows us to express our original matrix as a linear combination of low-rank matrices.<br>SVD deals with decomposing a matrix into a product of 3 matrices.<br>The SVD can be calculated by calling the svd() function.<br>The function takes a matrix and returns the U, Sigma and V^T elements. The Sigma diagonal matrix is returned as a vector of singular values. The V matrix is returned in a transposed form.<br>If the dimensions of A are m x n:<br>U is an m x m matrix of Left Singular Vectors<br>S is an m x n rectangular diagonal matrix of Singular Values  arranged in decreasing order<br>V is an n x n matrix of Right Singular Vectors<br>Applications of Singular Value Decomposition (SVD) <br>Image Compression<br>Image Recovery<br>Eigenfaces<br>Spectral Clustering<br>Background Removal from Videos |**• n_components:** int, default = 2 Desired dimensionality of output data. Must be strictly less than the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended.<br>**• algorithm:** string, default = “randomized” SVD solver to use. <br>**• n_iter:** int, optional (default 5) Number of iterations for randomized SVD solver.|• Remove noise and redundant information thereby improving algorithm results.<br>• Preferrer in high dimentional dataset.|• All the categorical features are required to be converted into numerical features before PCA can be applied.<br>• If the data is strongly non-linear it may not work so well.<br>• Transformed data may be difficult to interpret.
|<a id='415' ></a>**PLSR**|PLS is also a feature reduction method but it offers a supervised alternative to Principal Components Regression. The new set of features are also the linear combinations of the original regressors, but in computing them, the method makes use of the target variab.As a result, the results of this technique not only explain the linear combinations of original features but the response variable as well. |**• n_components:** int, (default 2) Number of components to keep.<br>**• Scale:** boolean, (default True) whether to scale the data|• PLS regression takes into account the variability of the dependent variables.<br>• PLSR is more reliable when identifying relevant variables and their magnitudes of influence, especially in cases of small sample size and low tolerance.<br>• PLSR is especially useful when predictors are highly correlated. |• PLSR being more prone to overfitting <br>• All the categorical features are required to be converted into numerical features before PCA can be applied.
|<a id='416' ></a>**t-SNE**|It reduces the n numeric columns in the dataset to fewer dimensions m (m < n) based on nonlinear local relationships among the data points.  It models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points in the new lower dimensional space<br>Used for EDA, also as an input parameter for other classification & clustering algorithms.It is extensively applied in Image processing, NLP, genomic data and speech processing.<br>Use cases: Facial Expression Recognition, Identifying Tumor subpopulations (Medical Imaging), Text comparison using wordvec|**• n_components:** int; (default=2); Dimensions of embedded space.<br>**• perplexity:** float; (default=30); the number of nearest neighbors; t-SNE is quite insensitive to this parameter.<br>**• early_exaggeration:** float; (default=4); the space between natural clusters in the embedded space; The choice of this parameter is not critical. <br>**• learning_rate:** float; (default=1000); range=(100,1000); If the cost function increases during initial optimization, the early_exaggeration factor or learning rate might be too high. If the cost function gets stuck, in a bad local minimum, increasing the learning_rate helps sometimes.<br>**• n_iter:** int; (default=1000); minimum=200; maximum number of iterations for the optimization.<br>**• metric:** string or callable; (default='euclidean'); distance between instances in a feature array. string parameter is allowed by scipy.spacial.distance.pdist. A metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS If metric is precomputed.|• It works well when Data is strongly non-linear.<br>• t-SNE outputs provide better results than linear dimensionality reduction models like PCA. <br>• Used on all high dimensional datasets.|• All the categorical features are required to be converted into numerical features before PCA can be applied.<br>•It doesn’t always provide a similar output on successive runs.
|<a id='417' ></a>**Factor Analysis**|FA is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. <br>It extracts maximum common variance from all variables and puts them into a common score.<br>In FA, the observed variables are linear combinations of the unobserved variable or factor.<br>Factor analysis does not require factors to be orthogonal.<br>Factor analysis is widely utilized in market research, advertising, psychology, finance, and operation research. Market researchers use factor analysis to identify price-sensitive customers, identify brand features that influence consumer choice, and helps in understanding channel selection criteria for the distribution channel.|**• n_components:** int, Default- None Dimensionality of latent space, the number of components of X that are obtained after transform. If None, n_components is set to the number of features <br>**• tol:** float, Default- 0.01 Stopping tolerance for log-likelihood increase.|• The underlying factors are labelable and interpretable. <br>• It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence.|• Sensitive to outliers.<br>• It is used when there should not be perfect multicollinearity. <br>• There should not be homoscedasticity between the variables.
|<a id='418' ></a>**Isomap**|Isomap stands for isometric mapping. Isomap is a non-linear dimensionality reduction method based on the spectral theory which tries to preserve the geodesic distances in the lower dimension.<br>Isomap starts by creating a neighborhood network.<br>After that, it uses graph distance to the approximate geodesic distance between all pairs of points. And then, through eigenvalue decomposition of the geodesic distance matrix, it finds the low dimensional embedding of the dataset|**• n_neighbors:** integer number of neighbors to consider for each point.<br>**• n_components:** integer number of coordinates for the manifold <br>**• path_methodstring:** [‘auto’,’FW’,’D’] Method to use in finding shortest path.‘auto’ : attempt to choose the best algorithm automatically.‘FW’ : Floyd-Warshall algorithm.‘D’ : Dijkstra’s algorithm.|• It works well when Data is strongly non-linear. <br>• Transformed data is easy to interpret and visualize.|• Sensitive to noise and outliers <br>• Inefficient for large datasets.


[Go To Top](#Contents)

# <code style="background:yellow;color:black">Recommender System</code>		
Recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, 
text to read, products to buy or anything else depending on industries). To achieve this task, 
there exist two major categories of methods : collaborative filtering methods and content based methods.

It refers to a system that is capable of predicting the future preference of a set of items for a user, 
and recommend the top items. One key reason why we need a recommender system in modern society is that people
 have too much options to use from due to the prevalence of Internet.

Youtube, Netflix, Amazon, Pinterest, and long list of other internet products all rely on recommender systems to 
filter millions of contents and make personalized recommendations to their users.
  
Recommender systems can be loosely broken down into three categories: content based systems, collaborative filtering systems, 
and hybrid systems (which use a combination of the other two

|ALGORITHMS|DESCRIPTION|PROS|CONS|
|:-|:-|:-|:-|
|<a id='512' ></a>**Content based filtering** |A content based recommender works with data that the user provides, either explicitly (rating) or implicitly (clicking on a link). Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more accurate.|**User independence:** The content-based method only has to analyze the items and a single user’s profile for the recommendation, which makes the process less cumbersome. Content-based filtering would thus produce more reliable results with fewer users in the system. <br>**Transparency:** Collaborative filtering gives recommendations based on other unknown users who have the same taste as a given user, but with content-based filtering, items are recommended on a feature-level basis.<br>**No cold start:** As opposed to collaborative filtering, new items can be suggested before being rated by a substantial number of users. |Content based recommenders have their own limitations. They are not good at capturing inter-dependencies or complex behaviors. <br>**Limited content analysis:** If the content doesn’t contain enough information to discriminate the items precisely, the recommendation itself risks being imprecise. <br>**Over-specialization:** Content-based filtering provides a limited degree of novelty since it has to match up the features of a user’s profile with available items. In the case of item-based filtering, only item profiles are created and users are suggested items similar to what they rate or search for, instead of their past history.
|<a id='513' ></a>**Collaborative filtering** |Collaborative filtering models which are based on assumption that people like things similar to other things they like, and things that are liked by other people with similar taste.Types of collaborative filtering techniques <br>** user based ** <br>** item based ** |**Not required to understand item content:** The content of the items does not necessarily tell the whole story, such as movie type/genre, and so on.<br>**Captures the change in user interests over time:** Focusing solely on content does not provide any flexibility on the user's perspective and their preferences.<br>**Captures inherent subtle characteristics:** This is very true for latent factor models. If most users buy two unrelated items, then it is likely that another user who shares similar interests with other users is highly likely to buy that unrelated item. |The system does not analyze the content <br>**Memory-based** algorithms present serious scalability problems given that the algorithm has to process all the data to compute a single prediction. With a high numberof users or items, these algorithms are not appropriate for online systems which recommend items in real time. <br>**Many models** are extremely complex, as they have a multitude of parameters to estimate, and many times they are too sensitive to data changes
|<a id='514' ></a>**Item based Collaborative filtering** |People who liked a particular product also liked these other products. Item based systems are item to item system. They generate recommendations based on similarity between items with respect to user rating of these items. Pearson correlation as driver for item based recommendation. |**Overcomes the problem of sparsity** of user rating as it takes into consideration of item features. Item-item models use rating distributions per item, not per user. <br>**Item-item collaborative filtering** had less error than user-user collaborative filtering. In addition, its less-dynamic model was computed less often and stored in a smaller matrix, so item-item system performance was better than user-user systems. |**Popularity bias:** recommender is prone to recommender popular items<br>**item cold-start problem:** recommender fails to recommend new or less-known items because items have either none or very little interactions
|<a id='515' ></a>**User based Collaborative filtering**|Recommend items based on similarity between users. Customers who are similar to you liked these products. User attributes if are similar, we can recommend similar items if one of them has liked it. User based collaborative filtering can be done by using logistic regression as a classifier|Context independent. <br> Compared to other techniques, such as content-based, it is more accurate|**Sparsity:** The percentage of people who rate items is really low.<br>**Scalability:** The more K neighbors we consider (under a certain threshold), the better my classification should be. Nevertheless, the more users there are in the system, the greater the cost of finding the nearest K neighbors will be.<br>**Cold-start:** New users will have no to little information about them to be compared with other users.<br>**New item:** Just like the last point, new items will lack of ratings to create a solid ranking.
|<a id='516' ></a>**Hybrid recommender system**|Hybrid filtering technique combines different recommendation techniques in order to gain better system optimization to avoid some limitations and problems of pure recommendation systems. <br>The idea behind hybrid techniques is that a combination of algorithms will provide more accurate and effective recommendations than a single algorithm as the disadvantages of one algorithm can be overcome by another algorithm.<br>The combination of approaches can be done in any of the following ways: separate implementation of algorithms and combining the result, utilizing some content-based filtering in collaborative approach, utilizing some collaborative filtering in content-based approach, creating a unified recommendation system that brings together both approaches. |Overcomes the disadvantages of multiple algorithms like cold start, data sparsity, accuracy etc.|Can be ambiguos to select the hybridization tehnique as multiple hybrid models are present.


[Go To Top](#Contents)
						
```python

```
